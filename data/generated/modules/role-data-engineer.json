{
  "slug": "role-data-engineer",
  "type": "role",
  "title": "Data Engineer Interview Guide",
  "description": "Prepare for data pipelines, warehousing, and infrastructure interviews.",
  "role_slug": "data-engineer",
  "is_premium": false,
  "display_order": 10,
  "sections": [
    {
      "id": "overview",
      "title": "Role Overview",
      "blocks": [
        {
          "type": "text",
          "content": {"text": "Data Engineering interviews focus on building and maintaining data infrastructureâ€”pipelines, warehouses, and processing systems. Expect SQL deep dives, system design for data, and coding challenges."}
        },
        {
          "type": "text",
          "content": {"text": "**What interviewers assess:**\n\n- SQL expertise (complex queries, optimization)\n- Data pipeline design and orchestration\n- Data modeling and warehouse design\n- Big data technologies (Spark, Kafka, etc.)\n- Programming skills (Python, Scala)"}
        }
      ]
    },
    {
      "id": "format",
      "title": "Common Interview Format",
      "blocks": [
        {
          "type": "text",
          "content": {"text": "**Typical data engineering interview loop:**\n\n1. **Recruiter screen** (30 min)\n2. **Technical phone screen** (60 min) - SQL or coding\n3. **Onsite/Virtual loop** (4-5 hours):\n   - SQL deep dive round\n   - Data pipeline design\n   - Coding round\n   - Behavioral round"}
        },
        {
          "type": "tip",
          "content": {"text": "Be ready to discuss data quality, testing strategies, and how you handle schema evolution and backward compatibility."}
        }
      ]
    },
    {
      "id": "competencies",
      "title": "Key Competencies",
      "blocks": [
        {
          "type": "checklist",
          "content": {
            "title": "Technical Areas to Review",
            "items": [
              {"id": "c1", "text": "Advanced SQL (window functions, CTEs, query optimization)", "required": true},
              {"id": "c2", "text": "Data modeling (star schema, slowly changing dimensions)", "required": true},
              {"id": "c3", "text": "ETL/ELT pipeline design", "required": true},
              {"id": "c4", "text": "Orchestration tools (Airflow, Dagster, Prefect)", "required": true},
              {"id": "c5", "text": "Big data processing (Spark, distributed systems)", "required": true},
              {"id": "c6", "text": "Streaming vs batch processing trade-offs", "required": true}
            ]
          }
        }
      ]
    },
    {
      "id": "preparation",
      "title": "Preparation Checklist",
      "blocks": [
        {
          "type": "checklist",
          "content": {
            "title": "Before Your Interview",
            "items": [
              {"id": "p1", "text": "Practice complex SQL on StrataScratch or LeetCode", "required": true},
              {"id": "p2", "text": "Review data warehouse design principles", "required": true},
              {"id": "p3", "text": "Prepare to design a data pipeline end-to-end", "required": true},
              {"id": "p4", "text": "Brush up on Python and data processing libraries", "required": true},
              {"id": "p5", "text": "Understand the company's data stack if possible", "required": true},
              {"id": "p6", "text": "Prepare examples of data quality issues you've solved", "required": true}
            ]
          }
        }
      ]
    }
  ]
}
